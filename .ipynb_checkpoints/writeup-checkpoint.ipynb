{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Simulator — Behavioral Cloning (P3)\n",
    "---\n",
    "\n",
    "![](images/1_UaIyt_RXPQBgpkZK2HX1Ow.png)\n",
    "\n",
    "The task is to implement a Deep Neural Network model that predicts the steering angle of a car given images of the road. We are provided with a simulator that can be operated in 2 modes — training and autonomous mode.\n",
    "\n",
    "In training mode, we drive the car around a track and save the frames (.jpg files) as well as the corresponding steering angle (stored in a .csv file). In fact other parameters values are recorded: throttle, velocity, brake. In autonomous mode, the trained model is used to predict the steering angle and drive the car.\n",
    "\n",
    "I used the training dataset provided by Udacity, available here. The dataset includes images of the road recorded by 3 cameras mounted on the front of the car, and the csv file with steering angle values for each frame/image.\n",
    "\n",
    "\n",
    "You can find most of the image effect (image augmentation in `dataset.py` file). The trained model was tested on on track1. Following  animations show the performance of my final model.\n",
    "\n",
    "Training \n",
    "-------\n",
    "![training_img](track1.mp4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals and Objective\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "- Use the simulator to collect data of good driving behavior\n",
    "- Build, a convolution neural network in Keras that predicts steering angles from images\n",
    "- Train and validate the model with a training and validation set\n",
    "- Test that the model successfully drives around track one without leaving the road\n",
    "- Summarize the results with a written report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "\n",
    "This project requires **Python 3.5** and the following Python libraries installed:\n",
    "\n",
    "- [Keras](https://keras.io/)\n",
    "- [Pandas](http://pandas.pydata.org/)\n",
    "- [OpenCV](http://opencv.org/)\n",
    "- [Matplotlib](http://matplotlib.org/) (Optional)\n",
    "- [Jupyter](http://jupyter.org/) (Optional)\n",
    "- [NumPy](http://www.numpy.org/)\n",
    "- [SciPy](https://www.scipy.org/)\n",
    "- [TensorFlow](http://tensorflow.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Structure\n",
    "\n",
    "My project includes the following files:\n",
    "* `model.py` containing the script to create, train, test, validate and save the model.\n",
    "* `drive.py` for driving the car in autonomous mode\n",
    "* `dataset.py` for image augmentation (resize, crop, shering .. etc)\n",
    "* `model.h5` containing a trained convolution neural network \n",
    "* `model.ipynb` containing data preprocessing, create, train, test, validate and save the model\n",
    "* this wirteup.md, [this article](https://medium.com/@ksakmann/behavioral-cloning-make-a-car-drive-like-yourself-dc6021152713) and image_transformation_pipeline.ipynb for explanation.\n",
    "\n",
    "Additionally you need to download and unpack the [Udacity self-driving car simulator](https://github.com/udacity/self-driving-car-sim). \n",
    "\n",
    "To run the model start the simulator in `autonomous mode`, open another shell and type \n",
    "```\n",
    "python drive.py model.h5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General considerations\n",
    "\n",
    "The simulated car is equipped with three cameras, one to the left, one in the center and one to the right of the driver that provide images from these different view points. The training track has sharp corners, exits, entries, bridges, partially missing lane lines and changing light conditions. An additional test track exists with changing elevations, even sharper turns and bumps. It is thus crucial that the CNN does not merely memorize the first track, but generalizes to unseen data in order to perform well on the test track. The model developed here was trained exclusively on the training track and completes the test track.\n",
    "\n",
    "The main problem lies in the skew and bias of the data set. Shown below is a histogram of the steering angles recorded while driving in the middle of the road for a few laps. This is also the data used for training. The left-right skew is less problematic and can be eliminated by flipping images and steering angles simultaneously. However, even after balancing left and right angles most of the time the steering angle during normal driving is small or zero and thus introduces a bias towards driving straight. The most important events however are those when the car needs to turn sharply. \n",
    "\n",
    "Without accounting for this bias towards zero, the car leaves the track quickly. One way to counteract this problem is to  purposely let the car drift  towards the side of the road and to start recovery in the very last moment. \n",
    "\n",
    "However, the correct large steering angles are not easy to generate this way, because even then most of the time the car drives straight, with the exception of the short moment when the driver avoids a crash or the car going off the road."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "### Image visuallization\n",
    "\n",
    "The dataset is made of a total of 24,108 color images (for left/center/right camera) of size (160 x 320) px\n",
    "![](images/image_visualization.png)\n",
    "\n",
    "### Data visuallization\n",
    "![](images/driving_log_csv.png)\n",
    "\n",
    "As shown above, the data file contains seven columns: URLs for the captured center, left, and right images; the steering angle; the throttle rate; braking; and speed. Each row captures a snapshot of what was happening in the simulator.\n",
    "\n",
    "![](images/data_summary.png)\n",
    "\n",
    "The summary statistics above show some interesting information about the data. Steering angle: While the steering angle varied between -1 and 1, it was almost always at 0. The mean is slightly to the right, which suggests additional recovery training was done to learn how to recover from the mostly left turning track. The throttle was almost always fully engaged, only occassionally being released to allow the vehicle to slow down. The break was only applied very lightly while driving. The car was driven almost exclusively at its full speed of 30 mph. It will be interesting to see how successfully the model will be when trained with this high velocity driving and whether it will generalize to slower driving speeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### steering angle profile:\n",
    "Below is a typical profile of the steering angle for successive frames :\n",
    "![](images/steering_angle_profile.png)\n",
    "\n",
    "Steering angle s=0 have the highest frequency: more than 20 times larger than the frequency of other angle values. Also, there are more positive (1900 counts) than negative angle values (1775 counts).\n",
    "![](images/steering_angle_distribution.png)\n",
    "\n",
    "Similarly to a classification problem, it’s important to check that the dataset is balance — for example, the frequency of negative angles has to be about the same as that of the positive angles. Without balancing the dataset, the model will have the strong bias of mostly predicting zero or negative steering angles. Several data augmentation schemes were built to balance the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "- We apply random shear operation. However, we select images with 0.9 probability for the random shearing process. \n",
    "![](images/random_sheared.png)\n",
    "\n",
    "- To help the system avoid learning other part of the image but only the track, user crops out the sky and car deck parts in the image. Original image size (160x320), after cropping 60px on top and 20px on the bottom, new image size is (80x320).\n",
    "![](images/random_cropped.png)\n",
    "\n",
    "- The next stage of the data processing pipeline is called random flip stage. In this stage we randomly (with 0.5 probability) flip images. The idea behind this operation is left turning bends are more prevalent than right bends in the training track.\n",
    "![](images/random_flip_brighten.png)\n",
    "\n",
    "- To help running a smaller training model, images are scaled to (64x64) size from cropped size (80x320)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "\n",
    "My convolutional neural network (CNN) architecture was inspired by NVIDIA's End to End Learning for Self-Driving Cars paper. The main difference between my model and the NVIDIA mode is that I did use MaxPooling layers just after each  Convolutional Layer in order to cut down training time. \n",
    "\n",
    "- 1st layer: normalize input image to -0.5 to 0.5 range.\n",
    "\n",
    "- For optimizer, Adam optimizer is used. I started with 0.001 training rate but 0.0001 seems to produce a smoother ride. Therefore, I kept 0.0001 learning rate.\n",
    "\n",
    "```\n",
    "____________________________________________________________________________________________________\n",
    "Layer (type)                     Output Shape          Param #     Connected to                     \n",
    "====================================================================================================\n",
    "lambda_2 (Lambda)                (None, 64, 64, 3)     0           lambda_input_2[0][0]             \n",
    "____________________________________________________________________________________________________\n",
    "convolution2d_6 (Convolution2D)  (None, 32, 32, 24)    1824        lambda_2[0][0]                   \n",
    "____________________________________________________________________________________________________\n",
    "activation_10 (Activation)       (None, 32, 32, 24)    0           convolution2d_6[0][0]            \n",
    "____________________________________________________________________________________________________\n",
    "maxpooling2d_6 (MaxPooling2D)    (None, 31, 31, 24)    0           activation_10[0][0]              \n",
    "____________________________________________________________________________________________________\n",
    "convolution2d_7 (Convolution2D)  (None, 16, 16, 36)    21636       maxpooling2d_6[0][0]             \n",
    "____________________________________________________________________________________________________\n",
    "activation_11 (Activation)       (None, 16, 16, 36)    0           convolution2d_7[0][0]            \n",
    "____________________________________________________________________________________________________\n",
    "maxpooling2d_7 (MaxPooling2D)    (None, 15, 15, 36)    0           activation_11[0][0]              \n",
    "____________________________________________________________________________________________________\n",
    "convolution2d_8 (Convolution2D)  (None, 8, 8, 48)      43248       maxpooling2d_7[0][0]             \n",
    "____________________________________________________________________________________________________\n",
    "activation_12 (Activation)       (None, 8, 8, 48)      0           convolution2d_8[0][0]            \n",
    "____________________________________________________________________________________________________\n",
    "maxpooling2d_8 (MaxPooling2D)    (None, 7, 7, 48)      0           activation_12[0][0]              \n",
    "____________________________________________________________________________________________________\n",
    "convolution2d_9 (Convolution2D)  (None, 7, 7, 64)      27712       maxpooling2d_8[0][0]             \n",
    "____________________________________________________________________________________________________\n",
    "activation_13 (Activation)       (None, 7, 7, 64)      0           convolution2d_9[0][0]            \n",
    "____________________________________________________________________________________________________\n",
    "maxpooling2d_9 (MaxPooling2D)    (None, 6, 6, 64)      0           activation_13[0][0]              \n",
    "____________________________________________________________________________________________________\n",
    "convolution2d_10 (Convolution2D) (None, 6, 6, 64)      36928       maxpooling2d_9[0][0]             \n",
    "____________________________________________________________________________________________________\n",
    "activation_14 (Activation)       (None, 6, 6, 64)      0           convolution2d_10[0][0]           \n",
    "____________________________________________________________________________________________________\n",
    "maxpooling2d_10 (MaxPooling2D)   (None, 5, 5, 64)      0           activation_14[0][0]              \n",
    "____________________________________________________________________________________________________\n",
    "flatten_2 (Flatten)              (None, 1600)          0           maxpooling2d_10[0][0]            \n",
    "____________________________________________________________________________________________________\n",
    "dense_6 (Dense)                  (None, 1164)          1863564     flatten_2[0][0]                  \n",
    "____________________________________________________________________________________________________\n",
    "activation_15 (Activation)       (None, 1164)          0           dense_6[0][0]                    \n",
    "____________________________________________________________________________________________________\n",
    "dense_7 (Dense)                  (None, 100)           116500      activation_15[0][0]              \n",
    "____________________________________________________________________________________________________\n",
    "activation_16 (Activation)       (None, 100)           0           dense_7[0][0]                    \n",
    "____________________________________________________________________________________________________\n",
    "dense_8 (Dense)                  (None, 50)            5050        activation_16[0][0]              \n",
    "____________________________________________________________________________________________________\n",
    "activation_17 (Activation)       (None, 50)            0           dense_8[0][0]                    \n",
    "____________________________________________________________________________________________________\n",
    "dense_9 (Dense)                  (None, 10)            510         activation_17[0][0]              \n",
    "____________________________________________________________________________________________________\n",
    "activation_18 (Activation)       (None, 10)            0           dense_9[0][0]                    \n",
    "____________________________________________________________________________________________________\n",
    "dense_10 (Dense)                 (None, 1)             11          activation_18[0][0]              \n",
    "====================================================================================================\n",
    "Total params: 2,116,983\n",
    "Trainable params: 2,116,983\n",
    "Non-trainable params: 0\n",
    " \n",
    "```\n",
    "\n",
    "\n",
    "### Training\n",
    "\n",
    "Even after cropping and resizing training images (with all augmented images), training dataset was very large and it could not fit into the main memory. Hence, we used `fit_generator` API of the Keras library for training our model.\n",
    "\n",
    "We created two generators namely:\n",
    "\n",
    "* `train_batch = Dataset().next_batch()`\n",
    "* `validation_batch = Dataset().next_batch()` \n",
    "\n",
    "Batch size of both `train_batch` and `validation_batch` was 64. We used 20032 images per training epoch. It is to be noted that these images are generated on the fly using the document processing pipeline described above. In addition to that, we used 6400 images (also generated on the fly) for validation. We used `Adam` optimizer with `1e-4` learning rate. Finally, when it comes to the number of training epochs we tried several possibilities such as `5`, `8`, `1`0, `2`5 and `50`. However, `6` works well on both training and validation tracks.\n",
    "\n",
    "![](images/model_train.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and conclusion\n",
    "\n",
    "Summarizing, this was a really interesting project. By making consequent use of image augmentation with according steering angle updates we could train a neural network to recover the car from extreme events, like suddenly appearing curves change of lighting conditions by exclusively simulating such events from regular driving data. \n",
    "\n",
    "There is much more that can be done. My next step is to tweak the Neural Net so that it is able to predict both the steering angle and the throttle opening.\n",
    "\n",
    "For more details please check out the code here:\n",
    "https://github.com/geekay2015/CarND-Behavioral-Cloning-P3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
